\relax 
\citation{chandola2009anomaly,iturbe2017towards}
\citation{chandola2009anomaly,iturbe2017towards,prunella2023deep}
\citation{hoang2024graspability,cui2023survey,vu2024occlusion}
\citation{bergmann2020uninformed,hoang2019object,liu2020towards,hoang2022context}
\citation{ren2024steel,zhang2024defect}
\citation{chandola2009anomaly}
\citation{yang2019real}
\citation{tao2022deep,cui2023survey}
\citation{cui2023survey}
\citation{akcay2019ganomaly,yu2023unsupervised,liu2020towards}
\citation{bergmann2020uninformed,salehi2021multiresolution,wang2021student,cao2022informative}
\providecommand \oddpage@label [2]{}
\citation{bergmann2023anomaly,rudolph2023asymmetric,horwitz2023back,wang2023multimodal}
\citation{hoang2025attention,wang2023multimodal,hoang2024efficient}
\citation{rudolph2023asymmetric,hoang2024collision}
\citation{horwitz2023back,hoang2024object}
\citation{yu2023unsupervised,collin2021improved}
\citation{liu2020towards}
\citation{akcay2019ganomaly,perera2019ocgan}
\citation{bergmann2018improving}
\citation{chung2020unsupervised}
\citation{zhou2020encoding}
\citation{collin2021improved}
\newlabel{sec:relatedwork}{{}{2}}
\citation{liu2020towards,dehaene2020iterative}
\citation{dehaene2020iterative}
\citation{dehaene2020anomaly}
\citation{wang2020image}
\citation{ganokratanaa2020unsupervised,ganokratanaa2022video}
\citation{yan2021learning}
\citation{song2021anoseg}
\citation{liang2023omni}
\citation{rudolph2021same,wang2021student}
\citation{bergmann2020uninformed,salehi2021multiresolution,wang2021student,cao2022informative}
\citation{tax2004support,defard2021padim}
\citation{yi2021patch,zhang2021anomaly,hu2021semantic}
\citation{cohen2020sub,li2021anomaly,wan2021industrial,roth2022towards}
\citation{li2021anomaly,wan2021industrial,roth2022towards}
\citation{lee2022cfa,kim2023fapm,defard2021padim}
\citation{hoang2022voting,horwitz2022empirical,hoang2023grasp,horwitz2023back}
\citation{hoang2024multi}
\citation{horwitz2022empirical,wang2023multimodal}
\citation{bergmann2023anomaly}
\citation{rudolph2023asymmetric}
\citation{horwitz2023back}
\citation{wang2023multimodal}
\citation{pang2022masked}
\citation{qi2017pointnet}
\citation{wang2023multimodal}
\citation{dosovitskiy2020image}
\citation{te2020edge,wang2018non}
\citation{wang2018non}
\citation{defferrard2016convolutional,te2020edge}
\citation{wang2018non}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Overview of the proposed unsupervised anomaly detection framework. The method extracts 2D visual features from RGB images and 3D geometric features from point clouds. A Visual-to-Geometric Feature Reconstruction network predicts geometric features from visual inputs, and significant discrepancies indicate anomalies.\relax }}{5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:view}{{1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The proposed visual feature enhancement module $\mathcal  {M}_{FE}$ used in geometric feature reconstruction. The module \DIFdelbeginFL  {\color {red} \relax \fontsize  {7}{8pt}\selectfont  improves local feature representation through }\DIFdelendFL  \DIFaddbeginFL  {\color {blue} \sf  incorporates }\DIFaddendFL  non-local attention and graph convolutional networks \DIFaddbeginFL  {\color {blue} \sf  to improve the interaction between global and local features}\DIFaddendFL  .\relax }}{5}}
\newlabel{fig:NL}{{2}{5}}
\citation{te2020edge}
\citation{te2020edge}
\citation{roth2022towards}
\citation{bergmann2022mvtec,wang2023multimodal}
\citation{bergmann2022mvtec}
\citation{bergmann2022mvtec}
\citation{bergmann2022mvtec}
\citation{bergmann2022mvtec}
\citation{bergmann2022mvtec}
\citation{wang2023multimodal}
\citation{bergmann2022mvtec}
\newlabel{sec:evaluation}{{}{7}}
\citation{wang2023multimodal}
\citation{fischler1981random}
\citation{wang2023multimodal}
\citation{dosovitskiy2020image}
\citation{deng2009imagenet}
\citation{pang2022masked}
\citation{chang2015shapenet}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Qualitative results of anomaly detection \DIFaddbeginFL  {\color {blue} \sf  on MVTec 3D-AD dataset \cite  {bergmann2022mvtec}}\DIFaddendFL  . From top to bottom: input RGB images, point clouds, ground-truth anomaly segmentations, and anomaly maps obtained by our proposed method without and with the visual feature enhancement module $\mathcal  {M}_{FE}$. With $\mathcal  {M}_{FE}$, the anomaly is accurately located in row 5.\relax }}{8}}
\newlabel{fig:results1}{{3}{8}}
\citation{qi2017pointnet++}
\citation{pang2022masked}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces {\color {blue} \sf  Additional qualitative results of anomaly detection on MVTec 3D-AD dataset \cite  {bergmann2022mvtec}. From top to bottom: input RGB images, point clouds, ground-truth anomaly segmentations, and anomaly maps obtained by our proposed method without and with the visual feature enhancement module $\mathcal  {M}_{FE}$. With $\mathcal  {M}_{FE}$, the anomaly is accurately located in row 5.}\relax }}{9}}
\newlabel{fig:results2}{{4}{9}}
\citation{kingma2014adam}
\citation{bergmann2022mvtec,wang2023multimodal}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces {\color {blue} \sf  Additional qualitative results of anomaly detection on synthetic data. From top to bottom: input RGB images, the rendered depth and normal maps, ground-truth anomaly segmentations, and anomaly maps obtained by our proposed method without and with the visual feature enhancement module $\mathcal  {M}_{FE}$. With $\mathcal  {M}_{FE}$, the anomaly is accurately located in row 5.}\relax }}{10}}
\newlabel{fig:results3}{{5}{10}}
\citation{bergmann2022mvtec}
\citation{horwitz2022empirical}
\citation{roth2022towards}
\citation{defard2021padim}
\citation{bergmann2023anomaly}
\citation{rudolph2023asymmetric}
\citation{rudolph2021same}
\citation{wang2021student}
\citation{wang2023multimodal}
\citation{wang2023multimodal}
\citation{bergmann2022mvtec}
\citation{horwitz2022empirical}
\citation{roth2022towards}
\citation{defard2021padim}
\citation{bergmann2023anomaly}
\citation{rudolph2023asymmetric}
\citation{rudolph2021same}
\citation{wang2021student}
\citation{wang2023multimodal}
\citation{wang2023multimodal}
\citation{bergmann2022mvtec}
\citation{rudolph2023asymmetric}
\citation{wang2023multimodal}
\citation{horwitz2022empirical}
\citation{gudovskiy2022cflow}
\citation{roth2022towards}
\citation{defard2021padim}
\citation{bergmann2023anomaly}
\citation{wang2023multimodal}
\citation{wang2023multimodal}
\citation{bergmann2022mvtec}
\citation{horwitz2022empirical}
\citation{gudovskiy2022cflow}
\citation{roth2022towards}
\citation{defard2021padim}
\citation{bergmann2023anomaly}
\citation{wang2023multimodal}
\citation{wang2023multimodal}
\citation{bergmann2022mvtec}
\citation{bergmann2023anomaly}
\citation{wang2023multimodal}
\citation{bonfiglioli2022eyecandies}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  I-AUROC score for anomaly detection of all categories of MVTec-3D AD. Results FPFH \cite  {horwitz2022empirical}, PatchCore \cite  {roth2022towards}, PADiM \cite  {defard2021padim}, 3D-ST \cite  {bergmann2023anomaly}, AST \cite  {rudolph2023asymmetric}, DifferNet \cite  {rudolph2021same}, STFPM \cite  {wang2021student}, M3DM \cite  {wang2023multimodal} are obtained from \cite  {wang2023multimodal}, and the remaining methods from \cite  {bergmann2022mvtec}. Best results in {\fontencoding  {T1}\fontseries  {b}\selectfont  {b}old}, runner-ups \relax $\@@underline {\hbox {underlined}}\mathsurround \z@ $\relax .\relax }}{12}}
\newlabel{tab:1}{{1}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  AUPRO score for anomaly localization of all categories of MVTec-3D. Results of FPFH \cite  {horwitz2022empirical}, CFlow \cite  {gudovskiy2022cflow}, PatchCore \cite  {roth2022towards}, PADiM \cite  {defard2021padim}, 3D-ST \cite  {bergmann2023anomaly}, M3DM \cite  {wang2023multimodal} are obtained from \cite  {wang2023multimodal}, and the remaining methods from \cite  {bergmann2022mvtec}. Best results in {\fontencoding  {T1}\fontseries  {b}\selectfont  {b}old}, runner-ups \relax $\@@underline {\hbox {underlined}}\mathsurround \z@ $\relax .\relax }}{13}}
\newlabel{tab:2}{{2}{13}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Comparison of anomaly detection performance, inference speed, and memory usage on Simulation and the MVTec 3D-AD dataset. AUPRO(0.3) and AUPRO(0.05) represent area under the per-region overlap curve with False Positive Rate (FPR) thresholds of 0.3 and 0.05, respectively. Frame rate is measured in frames per second (FPS), and memory usage refers to the memory footprint during inference. Ours (-$\mathcal  {M}_{FE}$) refers to our method without the visual feature enhancement module.\relax }}{13}}
\newlabel{tab:extra_results}{{3}{13}}
\citation{bergmann2023anomaly}
\citation{wang2023multimodal}
\bibstyle{IEEEtran}
\bibdata{References}
\bibcite{chandola2009anomaly}{1}
\bibcite{iturbe2017towards}{2}
\bibcite{prunella2023deep}{3}
\bibcite{hoang2024graspability}{4}
\bibcite{cui2023survey}{5}
\bibcite{vu2024occlusion}{6}
\bibcite{bergmann2020uninformed}{7}
\bibcite{hoang2019object}{8}
\bibcite{liu2020towards}{9}
\newlabel{sec:Conclusion}{{}{14}}
\@writefile{toc}{\contentsline {section}{REFERENCES}{14}}
\bibcite{hoang2022context}{10}
\bibcite{ren2024steel}{11}
\bibcite{zhang2024defect}{12}
\bibcite{yang2019real}{13}
\bibcite{tao2022deep}{14}
\bibcite{akcay2019ganomaly}{15}
\bibcite{yu2023unsupervised}{16}
\bibcite{salehi2021multiresolution}{17}
\bibcite{wang2021student}{18}
\bibcite{cao2022informative}{19}
\bibcite{bergmann2023anomaly}{20}
\bibcite{rudolph2023asymmetric}{21}
\bibcite{horwitz2023back}{22}
\bibcite{wang2023multimodal}{23}
\bibcite{hoang2025attention}{24}
\bibcite{hoang2024efficient}{25}
\bibcite{hoang2024collision}{26}
\bibcite{hoang2024object}{27}
\bibcite{collin2021improved}{28}
\bibcite{perera2019ocgan}{29}
\bibcite{bergmann2018improving}{30}
\bibcite{chung2020unsupervised}{31}
\bibcite{zhou2020encoding}{32}
\bibcite{dehaene2020iterative}{33}
\bibcite{dehaene2020anomaly}{34}
\bibcite{wang2020image}{35}
\bibcite{ganokratanaa2020unsupervised}{36}
\bibcite{ganokratanaa2022video}{37}
\bibcite{yan2021learning}{38}
\bibcite{liang2023omni}{39}
\bibcite{rudolph2021same}{40}
\bibcite{tax2004support}{41}
\bibcite{defard2021padim}{42}
\bibcite{yi2021patch}{43}
\bibcite{zhang2021anomaly}{44}
\bibcite{hu2021semantic}{45}
\bibcite{li2021anomaly}{46}
\bibcite{wan2021industrial}{47}
\bibcite{roth2022towards}{48}
\bibcite{lee2022cfa}{49}
\bibcite{kim2023fapm}{50}
\bibcite{hoang2022voting}{51}
\bibcite{horwitz2022empirical}{52}
\bibcite{hoang2023grasp}{53}
\bibcite{hoang2024multi}{54}
\bibcite{pang2022masked}{55}
\bibcite{qi2017pointnet}{56}
\bibcite{dosovitskiy2020image}{57}
\bibcite{te2020edge}{58}
\bibcite{wang2018non}{59}
\bibcite{defferrard2016convolutional}{60}
\bibcite{bergmann2022mvtec}{61}
\bibcite{fischler1981random}{62}
\bibcite{deng2009imagenet}{63}
\bibcite{chang2015shapenet}{64}
\bibcite{qi2017pointnet++}{65}
\bibcite{kingma2014adam}{66}
\bibcite{gudovskiy2022cflow}{67}
\bibcite{bonfiglioli2022eyecandies}{68}
\@writefile{toc}{\contentsline {subsection}{Dinh-Cuong Hoang}{16}}
\@writefile{toc}{\contentsline {subsection}{Phan Xuan Tan}{16}}
\@writefile{toc}{\contentsline {subsection}{Anh-Nhat Nguyen}{16}}
\@writefile{toc}{\contentsline {subsection}{Duc-Thanh Tran}{16}}
\@writefile{toc}{\contentsline {subsection}{Van-Hiep Duong}{16}}
\@writefile{toc}{\contentsline {subsection}{Anh-Truong Mai}{17}}
\@writefile{toc}{\contentsline {subsection}{Duc-Long Pham}{17}}
\@writefile{toc}{\contentsline {subsection}{Khanh-Toan Phan}{17}}
\@writefile{toc}{\contentsline {subsection}{Minh-Quang Do}{17}}
\@writefile{toc}{\contentsline {subsection}{Ta Huu Anh Duong}{17}}
\@writefile{toc}{\contentsline {subsection}{Tuan-Minh Huynh}{17}}
\@writefile{toc}{\contentsline {subsection}{Son-Anh Bui}{17}}
\@writefile{toc}{\contentsline {subsection}{Duc-Manh Nguyen}{17}}
\@writefile{toc}{\contentsline {subsection}{Viet-Anh Trinh}{17}}
\@writefile{toc}{\contentsline {subsection}{Khanh-Duong Tran}{17}}
\@writefile{toc}{\contentsline {subsection}{{\color {blue} \sf  Thu-Uyen Nguyen}}{18}}
